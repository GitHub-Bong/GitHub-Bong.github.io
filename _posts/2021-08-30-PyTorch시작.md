---
title:  "2021-08-30 PyTorch 시작"
excerpt: "파이토치 잘 쓰고 싶다."

categories:
  - TIL
tags:
  - PyTorch
---
<pre>
<code>
# Import
import torch
from torch import nn
from torchvision import datasets
from torch.utils.data import DataLoader
from torchvision.transforms import transforms
import torch.nn.functional as F

# GPU check
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(device)

# Dataload
batch_size = 64

transform = transforms.Compose(
[transforms.ToTensor(),
transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]
)
train_data = datasets.CIFAR10(root='data', train=True, download=True, transform=transform)
train_loader = DataLoader(train_data, batch_size=64, shuffle=True)
test_data = datasets.CIFAR10(root='data', train=False, download=True, transform=transform)
test_loader = DataLoader(test_data, batch_size=64, shuffle=True)

classes = ('plane', 'car', 'bird', 'cat',
'deer', 'dog', 'frog', 'horse', 'ship', 'truck')


# Network
class NewNetwork(nn.Module): # input [64, 3, 32, 32]
def __init__(self):
super(NewNetwork, self).__init__()
self.conv1 = nn.Conv2d(3, 64, 3)
self.conv2 = nn.Conv2d(64, 128, 3)
# self.conv3 = nn.Conv2d(64, 128, 3)

self.pool1 = nn.MaxPool2d((2,2))


self.pool2 = nn.AdaptiveMaxPool2d(2)

self.fc1 = nn.Linear(128 * 6 * 6, 1024)
self.fc2 = nn.Linear(1024, 256)
self.fc3 = nn.Linear(256, 10)

self.dropout1 = nn.Dropout(0.3)

self.softmax1 = nn.Softmax(1)

def forward(self, x):
x = self.dropout1(F.relu(self.conv1(x)))
x = self.pool1(x)
x = self.dropout1(F.relu(self.conv2(x)))
x = self.pool1(x)
# x = self.dropout1(F.relu(self.conv3(x)))
# x = self.pool2(x)
x = torch.flatten(x, 1)
x = self.dropout1(F.relu(self.fc1(x)))
x = self.dropout1(F.relu(self.fc2(x)))
x = self.fc3(x)

return x

model = NewNetwork()
model.to(device)

# Loss, Optimizer, Epochs
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters())
epochs = 10

# Training
def trainloop(dataloader, model, loss_fn, optimizer):
size = len(dataloader.dataset)
running_loss = 0.0

for batch, data in enumerate(dataloader):
X, y = data[0].to(device), data[1].to(device)
pred = model(X)
loss = loss_fn(pred, y)

optimizer.zero_grad()
loss.backward()
optimizer.step()

running_loss += loss.item()

if batch % 200 == 0:
loss, current = running_loss / 200, batch * 64
print(f"loss: {loss:>.4f}, current: [{current:>6d}/{size:>6d}]")
running_loss = 0.0

def testloop(dataloader, model, loss_fn):
size = len(dataloader.dataset)
test_loss, correct = 0, 0

with torch.no_grad():
for data in dataloader:
X, y = data[0].to(device), data[1].to(device)
pred = model(X)
test_loss += loss_fn(pred, y).item()
correct += (pred.argmax(1) == y).type(torch.float).sum().item()

test_loss /= len(dataloader)
correct /= size
print(f"Test loss: {test_loss:>.3f}, Test Accuracy: {(correct*100):>.3f}% \n")

from torch.utils.tensorboard import SummaryWriter
writer = SummaryWriter('runs/cifar_experiment_1')
data = iter(train_loader).next()
images, labels = data[0].to(device), data[1].to(device)
writer.add_graph(model, images)
writer.close()

for t in range(epochs):
print(f"{(t+1):>2d}th epochs")

running_loss = 0.0
for batch, data in enumerate(train_loader):
X, y = data[0].to(device), data[1].to(device)
pred = model(X)
loss = loss_fn(pred, y)

optimizer.zero_grad()
loss.backward()
optimizer.step()

running_loss += loss.item()

if batch % 200 == 0:
loss, current = running_loss / 200, batch * 64
print(f"loss: {loss:>.4f}, current: [{current:>6d}/{len(train_loader.dataset):>6d}]")
running_loss = 0.0

test_loss, correct = 0, 0
with torch.no_grad():
for data in test_loader:
X, y = data[0].to(device), data[1].to(device)
pred = model(X)
test_loss += loss_fn(pred, y).item()
correct += (pred.argmax(1) == y).type(torch.float).sum().item()

test_loss /= len(test_loader)
correct /= len(test_loader.dataset)
print(f"Test loss: {test_loss:>.3f}, Test Accuracy: {(correct * 100):>.3f}% \n")
# torch.save(model, 'model.pth')
# model = torch('model.pth')
</code>
</pre>